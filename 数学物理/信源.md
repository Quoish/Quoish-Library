#信息论 #随机过程
# 信源的数学模型
## 随机变量模型
### 简单离散信源
>如果信源输出的消息数量是有限或者可数的，而且每次只输出符号集中的一个消息，这样的信源称为*简单离散信源*。

简单离散信源是无记忆的，可以用一维离散随机变量加以描述，假设信源符号数量为$n$，这种信源的数学模型可以用下列离散概率空间加以描述。
$$\begin{bmatrix}
X \\
p(x)
\end{bmatrix}=
\begin{bmatrix}
a_{1} & a_{2} & \dots & a_{n} \\
p(a_{1}) & p(a_{2}) & \dots & p(a_{n})
\end{bmatrix}$$
且满足
$$\sum_{i=1}^n p(a_{i})=1$$
### 简单连续信源
>如果信源的输出是单个符号消息，但是消息的数量是不可数的，即输出消息的取值是连续的，这样的信源称为*简单连续信源*。

简单连续信源可以使用一维连续随机变量来描述消息，对应的数学模型为连续概率空间：
$$\begin{bmatrix}
X \\
p(x)
\end{bmatrix}=
\begin{bmatrix}
(a,b) \\
p(x)
\end{bmatrix}
\quad 或\quad
\begin{bmatrix}
X \\
p(x)
\end{bmatrix}=
\begin{bmatrix}
\mathbb{R} \\
p(x)
\end{bmatrix}$$
且满足
$$\int_{a}^b p(x)\mathrm{d}x=1\quad或\quad \int_{\mathbb{R}}p(x)\mathrm{d}x=1$$
## 随机矢量模型
## 多维离散信源
>如果离散信源输出的消息是由一系列符号组成的，这样的信源称为*多维离散信源*。

这类信源不能使用一维随机变量进行描述，应当使用$n$维随机矢量$X=(X_{1},X_{2},\dots,X_{n})$描述，$n$维随机矢量也称为$n$维随机序列。
一般说来，随机序列的统计特性比较复杂，分析起来比较困难。如果信源输出的随机序列的统计特性与时间的推移无关，那么该序列是平稳的。平稳随机序列分析相对简单，在实际中，为了分析问题方便起见，假设分析的序列是平稳的。
>如果信源输出的随机序列中，每个随机变量都是离散的，而且随机矢量的各维概率分布都与时间无关，即任何时刻随机矢量的各维概率分布相同，那么这样的信源称为*离散平稳信源*，可以用$n$维概率空间描述。

如果$n$维随机矢量$X$中的每个随机变量$X_{i}$($i=1,2,\dots,N$)都是离散的，且每个变量$X_{i}$的取值$x_i$都来自于符号集$A=\{a_{1},a_{2},\dots,a_{n}\}$，那么信源的$n$重概率空间为
$$\begin{bmatrix}
\vec{X} \\
\vec{p}(\vec{x})
\end{bmatrix}=
\begin{bmatrix}
(a_{1},a_{1},\dots,a_{1})  & (a_{1},a_{1},\dots,a_{2}) & \dots & (a_{n},a_{n},\dots,a_{n})\\
\vec{p}(a_{1},a_{1},\dots,a_{1}) & \vec{p}(a_{1},a_{1},\dots,a_{2}) & \dots & \vec{p}(a_{n},a_{n},\dots,a_{n})
\end{bmatrix}$$
## 多维连续信源
>如果连续信输出消息时由一系列符号组成，这样的信源称为*多维连续信源*，也可以用$n$维随机矢量$X=(X_{1},X_{2},\dots,X_{n})$来描述。
>如果信源输出的$n$维随机矢量中，每个随机变量都是离散的，而且随机矢量的各维概率分布都与时间无关，即任何时刻随机矢量的各维概率分布相同，那么这样的信源称为*连续平稳信源*。
## 离散无记忆信源
>若信源发出消息的符号之间彼此独立同分布，则称这样的信源为*离散无记忆信源*。

在此情况下，可以根据概率论理论将N维联合概率分布表示为
$$P(\vec{X})=\prod_{i=1}^N p(X_{i})=\prod_{i=1}^N p(a_{k_{i}})$$
其中，$k_{i}$可取$1,2,\dots,n$。
## 离散有记忆信源
通常情况下，信源在不同时刻发出的符号之间是相互关联的。
在实际中，这类信源很多。如由汉字组成的中文序列中，构成一段文字的汉字之间存在关联性，这样才能符合中文语法、习惯用语、表达实际意义等制约的要求。
这种离散有记忆信源，需要使用N维联合概率分布加以描述，通过引入[[条件概率]]可以说明符号之间的依赖关系，或者反映各符号之间的记忆特征。
$$
\begin{align}
p(x_{1}x_{2}\dots x_{N})&=p(x_{N}|x_{1}x_{2}\dots x_{N-1})p(x_{1}x_{2}\dots x_{N-1}) \\
&=p(x_{N}|x_{1}x_{2}\dots x_{N-1})p(x_{N-1}|x_{1}x_{2}\dots x_{N-2})p(x_{1}x_{2}\dots x_{N-2}) \\
&\dots
\end{align}$$
表述的复杂程度将随序列长度的增加而增加。而在实际信源中，符号之间的相关性随着符号间隔的增加而减弱，即信源发出的符号往往只与前若干符号的依赖性较强，而与更前面的符号依赖性很弱。这样就可以根据实际研究的需要限制随机序列的长度，有时更是需要考虑系统复杂程度来建立更为简单的模型，以达到相应的研究目的。
>当记忆长度为$m+1$时，即信源每次发出符号只与前$m$个符号相关，与更前面的符号无关，称这种信源为$m$阶*Markov信源*。

这时描述符号之间依赖关系的条件概率可以简化为
$$p(x_{i}|x_{i-1}x_{i-2}\dots x_{i-m}\dots)=p(x_{i}|x_{i-1}x_{i-2}\dots x_{i-m})$$
更进一步
$$p(x_{i}|x_{i-1}x_{i-2}\dots x_{i-m}\dots)=p(x_{m}|x_{m-1}x_{m-2}\dots x_{1})$$