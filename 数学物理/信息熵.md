#信息论 
# 平均信息量
## 信息熵
>自信息量的期望值称作*信息熵*。$$H(X)=E[I(a_{i})]=-\sum_{i=1}^r p(a_{i})\log p(a_{i})$$

信息熵表示信源的先验平均不确定性。表示消息符号发出前，表示确定信源$X$所需的平均[[信息量]]，或在发出消息后，每个输出消息符号所给出的[[信息量]]的平均值。
注意：上述信息熵的定义中可能包括不可能的符号。由于$\lim_{ p \to 0 }p\log p=0$，所以定义$p=0$时的$p\log p=0$
注注意：信息熵只和统计整体有关系，和符号形式没有关系，和符号排列顺序也没有关系
## 条件熵
>在联合集（$\{X,Y\}$）上，定义条件自信息量的数学期望$$H(X|Y)=E[I(a_{i}|b_{j})]=\sum_{i=1}^r \sum_{j=1}^s p(a_{i},b_{j})I(a_{i}|b_{j})$$
为集合$Y$相对于集合$X$的*条件熵*。

特别地，当$X,Y$相互独立时，有
$$H(X|Y)=H(X)$$
其意义是，观测到数据集合Y之后X的不确定度，有时也称为疑义度。
## 联合熵
>在联合集（$\{X,Y\}$）上，定义每一对元素$(a_{i},b_{j})$同时出现的联合自信息量的数学期望$$H(XY)=E[I(a_{i},b_{j})]=\sum_{i=1}^r \sum_{j=1}^s p(a_{i},b_{j})I(a_{i},b_{j})$$
为*联合熵*，也称共熵。

由Bayes公式可证：
$$H(XY)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$
## 交互熵
>在联合集（$\{X,Y\}$）上，定义互信息量$I(a_{i};b_{j})$的数学期望$$I(X;Y)=E[I(a_{i},b_{j})]=\sum_{i=1}^r \sum_{j=1}^s p(a_{i},b_{j})I(a_{i};b_{j})$$为*交互熵*，又称平均互信息量，互信息，平均交信息量。
## 各信息熵之间的关系

# 信息熵的性质
## 非负性
$$H(X)\geq 0$$
等号成立当且仅当$X$是确定事件。
## 对称性
调换$X$的各个符号$a_{i}$，熵的值保持不变。
即熵只和统计特性有关，和符号的具体含义和顺序无关。
## 确定性
如果信源的确定性很大，即使有其他的输出符号，只要有一个事件几乎必然出现，那么信息量就是确定的，其熵为0。
## 扩展性
当信源存在几乎为0的小概率事件时，尽管小概率事件能提供很大的信息量，不影响整体的熵，因为这个事件的平均信息量为0。
## 可加性
$$H(XY)=H(X)+H(Y|X)=H(Y)+H(X|Y)$$
若$X$和$Y$相互独立，则
$$H(XY)=H(X)+H(Y)$$
## 香农辅助定理
- 对于任意$r$维概率矢量$\vec{P}=(p_{1},p_{2},\dots p_{r})$，$\vec{Q}=(q_{1},q_{2},\dots q_{r})$，下列不等式成立：
$$H(\vec{P})=H[(p_{1},p_{2},\dots p_{r})]= -\sum_{i=1}^r p_{i}\log p_{i} \leq -\sum_{i=1}^r p_{i}\log q_{i}$$
该定理表明：给定任意概率分布$p_{i}$，对其他概率分布$q_{i}$的自信息量的数学期望，必然会大于信源本来的熵。
## 最大熵定理
离散无记忆信号源输出r个不同的信息符号，当且仅当各个输出符号的概率相等时，信源的熵最大。
$$H(X)\leq H\left[ \left( \frac{1}{r}, \frac{1}{r},\dots, \frac{1}{r} \right) \right]$$
其意义是，当信源符号分布相等时，很难猜出当前信源输出哪个符号，信源的不确定性最大。
## 条件熵小于无条件熵
$$H(X)\geq H(X|Y)$$
