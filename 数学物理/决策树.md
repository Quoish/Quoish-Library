#机器学习 #算法 
决策树（decision tree）是一种基本的分类与回归方法，决策树模型呈[[树]]形结构，在分类问题中，表示基于特征对实例进行分类的过程。
它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布，其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类。
决策树学习通常包括3个步骤：
1. 特征选择
2. 决策树的生成
3. 决策树的修剪
![[流程图形式的决策树.png]]
# 特征选择
影响决策树结构的关键因素是每一步的特征选择。
![[决策树案例1.png]]![[决策树案例2.png]]
决策树学习的核心就在于，寻找出能最快、准确地区分出样本类别的决策树。
依据[[信息熵]]，我们可以度量在每一次做出决策后，问题不确定性的下降程度，这里需要引入信息增益的概念。
## 信息增益
> 信息增益：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即
$$g(D,A)=H(D)-H(D|A)$$

信息增益计算方法：
设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。
设有$K$个类$C_{k},k=1,2,\dots,K$，$|C_{k}|$为属于类$C_{k}$的样本个数，$∑C_{k}=|D|$。
设特征$A$有$n$个不同的取值$\{a_{1},a_{2},\dots,a_{n}\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_{1},D_{2},\dots,D_{n},|D_{i}|$为$D_{i}$的样本个数，$\sum |D_{i}|=|D|$。
记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$，即$D_{ik}=D_{i}\cap C_{k}$，$|D_{ik}|$为$D_{ik}$的样本个数。
于是信息增益的算法如下：
- 输入：特征数据集$D$和特征集合$A$
- 输出：特征$A$对特征数据集$D$的信息增益$g(D,A)$
- 步骤：
1. 计算数据集$D$的经验熵$$H(D)=-\sum_{k=1}^K \frac{|C_{k}|}{|D|}\log_{2} \frac{|C_{k}|}{|D|}$$
2. 计算特征A对数据集D的经验条件熵$$H(D|A)=\sum_{i=1}^n \frac{|D_{i}|}{|D|}H(D_{i})=-\sum_{i=1}^n \frac{|D_{i}|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_{i}|}\log_{2} \frac{|D_{ik}|}{|D_{i}|}$$
3. 计算信息增益$$g(D,A)=H(D)-H(D|A)$$
## 信息增益比
- 信息增益比：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差与经验熵$H(D)$之比，即
$$g_{R}(D,A)=\frac{H(D)-H(D|A) }{H(D)}$$
# 决策树的生成
## ID3算法
- 输入：特征数据集$D$，特征集合$A$，阈值$\varepsilon$
- 输出：决策树$T$
- 步骤：
1. 若$D$中所有实例属于同一类$C_{k}$，则$T$为单节点树，并将类$C_{k}$作为该节点的类标记，返回$T$
2. 若$A=\varnothing$，则$T$为单节点树，并将实例中数量最大的类$C_{k}$作为该节点的类标记，返回$T$
3. 否则，计算$A$中各特征对D的【信息增益】，选择信息增益最大的特征$A_{k}$
	1. 如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置$T$为单节点树，并将实例中数量最大的类$C_{k}$作为该节点的类标记，返回$T$
	2. 否则，对于$A_{g}$的每一可能值$a_{i}$，依照$A_{g}=a_{i}$将$D$分隔为若干非空子集$D_{i}$，将$D_{i}$中实例数最大的类作为标记，构建子节点，由节点和子节点构成数$T$，返回$T$
	3. 对第i个子节点，以$D_{i}$为训练集，以$A-\{A_g\}$为特征集合，递归调用上述5步，得到子树$T_{i}$，返回$T_{i}$
## C4.5算法
- 输入：特征数据集$D$，特征集合$A$，阈值$\varepsilon$
- 输出：决策树$T$
- 步骤：
1. 若$D$中所有实例属于同一类$C_{k}$，则$T$为单节点树，并将类$C_{k}$作为该节点的类标记，返回$T$
2. 若$A=\varnothing$，则$T$为单节点树，并将实例中数量最大的类$C_{k}$作为该节点的类标记，返回$T$
3. 否则，计算$A$中各特征对D的【信息增益比】，选择信息增益最大的特征$A_{k}$
	1. 如果$A_{g}$的信息增益小于阈值$\varepsilon$，则置$T$为单节点树，，并将实例中数量最大的类$C_{k}$作为该节点的类标记，返回$T$
	2. 否则，对于$A_{g}$的每一可能值$a_{i}$，依照$A_{g}=a_{i}$将$D$分隔为若干非空子集$D_{i}$，将$D_{i}$中实例数最大的类作为标记，构建子节点，由节点和子节点构成数$T$，返回$T$
	3. 对第i个子节点，以$D_{i}$为训练集，以$A-\{A_g\}$为特征集合，递归调用上述5步，得到子树$T_{i}$，返回$T_{i}$
# 实现
```python
class TreeNode(object):  
    def __init__(self, value):  
        self.value = value
        self.children = []
    
    def add_child(self, child):
	    self.chidren.append(child)
	
	def del_child(self, child):
		self.chidren.remove(child)

class DecisionTree():
	
	def __init__(self):
		
		self.n_charas = 0
		self.n_labels = 0
		self.n_samples = 0
	
	def fit(self, chara_data, labels):
		self.n_samples, self.n_charas = chara_data.shape
		self.n_labels = labels.shape[0]
		tree = TreeNode(labels最多的标记)
		self.__grow_tree(tree, chara_data, labels, threshold)
		
	def __grow_tree(tree, chara_data, labels, threshold):
		if 所有样本同属一个label:
			tree.value = 该样本的类标记
			return tree
			
		if chara_data是空集:
			tree.value = 样本中数量最多的标记
			return tree
		
		else:
			info_gain_ratio = 计算信息增益比（chara_data, labels）
			max_info_gain_ratio = 最大的信息增益比.max()
			max_info_gain_ratio_idx = info_gain_ratio.index(max_info_gain_ratio)
			if max_info_gain_ratio.value() < threshold:
				tree.value = 样本中数量最多的标记
				return tree
			else:
				unique_charas = 将chara_data处于max_info_ratio_idx列的数据转成唯一的集合
				for unique_char in unique_charas:
					unique_char_idx = chara_data在max_info_ratio_idx列是unique_char的行数
					sub_tree = TreeNode(样本数最多的标记)
					tree.add_child(__grow_tree(
													subtree,
													chara_data只有unique_chara_idx的行, 
													labels只有unique_chara_idx的行，
													threshold,
													)
				
				
			
			
```