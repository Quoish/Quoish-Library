#信息论 
# 信息量
## 自信息量
>给定[[信源]]$X$的概率空间$$\begin{bmatrix}
X \\
p(x)
\end{bmatrix}=
\begin{bmatrix}
a_{1} & a_{2} & \dots & a_{n} \\
p(a_{1}) & p(a_{2}) & \dots & p(a_{n})
\end{bmatrix}$$
事件$a_{i}\in X$的*自信息量*定义为$$I(a_{i})=-\log_{b}p(a_{i})$$

该定义来源于以下原则：
1. 确定性事件，即$P=1$，信息量应该为0；
2. 事情的概率越小，信息量应该更大；
3. 自信息量是非负的；
4. 两个相互独立事件联合自信息量应该等于它们各自信息量之和。

自信息量的单位与所选取对数的底$b$有关。
比如如果在8个位置用红绿蓝三色旗来表示信息（8位三进制数据），则总共有$3^8$种事件，这时的底应该取为3。
数字系统中使用的是2进制，所以以2为底的情况是最常见的，此时信息量的单位是$\mathrm{Bit}$。
如果底取自然对数$e$，则单位为$\mathrm{nat}$。
如果底取10，则单位为$\mathrm{\det}$
## 互信息量
>对于给定的两个信源$$\begin{bmatrix}
X \\
p(x)
\end{bmatrix}=
\begin{bmatrix}
a_{1} & a_{2} & \dots & a_{r} \\
p(a_{1}) & p(a_{2}) & \dots & p(a_{r})
\end{bmatrix},\quad
\begin{bmatrix}
Y \\
p(y)
\end{bmatrix}=
\begin{bmatrix}
b_{1} & b_{2} & \dots & b_{s} \\
p(b_{1}) & p(b_{2}) & \dots & p(b_{s})
\end{bmatrix}$$
事件$b_{j}\in Y$的出现给出的关于事件$a_{i}\in X$的*互信息量*定义为$$I(a_{i};b_{j})=\log \frac{p(a_{i}|b_{j})}{p(a_{i})}$$
同样，定义事件$a_{i}\in X$的出现给出的关于事件$b_{j}\in Y$的信息量$$I(b_{j};a_{i})=\log \frac{p(b_{j}|a_{i})}{p(b_{j})}$$

可以证明：
$$I(a_{i};b_{j})=I(b_{j};a_{i})$$
由Bayes公式可得
$$I(a_{i};b_{j})=\log \frac{p(a_{i},b_{j})}{p(a_{i})p(b_{j})}$$
互信息量表示了事件的相关性
- 若事件$b_{j}$与事件$a_{i}$无关，则$I(a_{i};b_{j})=0$
-  若事件$b_{j}$有利于事件$a_{i}$的发生，则$I(a_{i};b_{j})>0$
- 若事件$b_{j}$不利于事件$a_{i}$的发生，则$I(a_{i};b_{j})<0$
## 条件自信息量
>给定联合概率空间$$\begin{bmatrix}
XY \\
p(x,y)
\end{bmatrix}=
\begin{bmatrix}
a_{1}b_{1} & a_{1}b_{2} & \dots & a_{r}b_{s} \\
p(a_{1},b_{1}) & p(a_{1},b_{2}) & \dots & p(a_{r},b_{s})
\end{bmatrix}$$
对于事件$a_{i}\in X$在事件$b_{j}\in Y$给定条件下的条件*自信息量*定义为$$I(a_{i}|b_{j})=-\log p(a_{i}|b_{j})$$

其意义是，在事件$b_{j}$给定的情况下，关于事件$a_{i}$的不确定性，这是从概率的意义上说的。
另一种解释是，在事件$b_{j}$给定的情况下，为了唯一确定$a_{i}$所必须提供的信息量。
由Bayes公式易得
$$I(a_{i})=I(a_{i};b_{j})+I(a_{i}|b_{j})$$
## 联合自信息量
>给定联合概率空间$$\begin{bmatrix}
XY \\
p(x,y)
\end{bmatrix}=
\begin{bmatrix}
a_{1}b_{1} & a_{1}b_{2} & \dots & a_{r}b_{s} \\
p(a_{1},b_{1}) & p(a_{1},b_{2}) & \dots & p(a_{r},b_{s})
\end{bmatrix}$$
对于事件$a_{i}\in X$在事件$b_{j}\in Y$给定条件下的*联合自信息量*定义为$$I(a_{i},b_{j})=-\log p(a_{i},b_{j})$$

其意义是，事件$a_{i}\in X$和事件$b_{j}\in Y$同时出现的先验不确定性。
如果$X,Y$表示信源连续输出的两个符号，则联合自信息量表示信源符号先后输出符号对$a_{i}b_{j}$提供的信息量。